{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etbbNkXBjJVv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Tokenizer BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#  O (Outside), B-PER (Beginning-Person), I-PER (Inside-Person), B-DIS (Disease), B-MED (Medication)\n",
        "label_map = {\n",
        "    \"O\": 0,\n",
        "    \"B-PER\": 1,\n",
        "    \"I-PER\": 2,\n",
        "    \"B-DIS\": 3,\n",
        "    \"B-MED\": 4,\n",
        "    \"I-DIS\": 5,\n",
        "    \"I-MED\": 6,\n",
        "}\n",
        "num_labels = len(label_map)\n",
        "\n",
        "num_labels = len(label_map)\n",
        "\n",
        "# Exemple\n",
        "texts = [\"Alex Smith has been diagnosed with Hypertension and prescribed Lisinopril.\",\n",
        "         \"Jordan Brown was given Metformin for Type 2 Diabetes.\", \"Marie Curie was treated with Radiotherapy for her condition.\",\n",
        "    \"Thomas Edison has been experiencing severe headaches and was prescribed Ibuprofen.\",\n",
        "    \"Nikola Tesla showed symptoms of insomnia and received Zolpidem.\",\n",
        "    \"Albert Einstein was diagnosed with Dyslexia and advised to follow special educational strategies.\",\n",
        "    \"Isaac Newton suffered from chronic pain and used Morphine.\"]\n",
        "\n",
        "labels = [[\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"B-DIS\", \"O\", \"O\", \"B-MED\", \"O\"],\n",
        "          [\"B-PER\", \"I-PER\", \"O\", \"O\", \"B-MED\", \"O\", \"B-DIS\", \"I-DIS\", \"O\"], [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"B-DIS\", \"O\", \"O\", \"O\"],\n",
        "    [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MED\", \"O\"],\n",
        "    [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"B-DIS\", \"O\", \"O\", \"B-MED\", \"O\"],\n",
        "    [\"B-PER\", \"I-PER\", \"O\", \"O\", \"B-DIS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n",
        "    [\"B-PER\", \"I-PER\", \"O\", \"O\", \"B-DIS\", \"O\", \"O\", \"B-MED\", \"O\"]]\n",
        "\n",
        "# Préparation des données\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, label_map, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_map = label_map\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        word_labels = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
        "        labels = [self.label_map[label] for label in word_labels] + [self.label_map[\"O\"]] * (self.max_len - len(word_labels))\n",
        "\n",
        "        item = {key: torch.tensor(val[0]) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.tensor(labels[:self.max_len], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "# Création du DataLoader\n",
        "dataset = NERDataset(texts, labels, tokenizer, label_map)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "from torch import nn\n",
        "\n",
        "class BertForNER(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super(BertForNER, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        # La dimension d'entrée de la couche linéaire est la taille cachée de BERT (768 pour bert-base-uncased)\n",
        "        self.classifier = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        logits = self.classifier(sequence_output)\n",
        "        return logits\n",
        "\n",
        "model = BertForNER(num_labels=num_labels)\n"
      ],
      "metadata": {
        "id": "Q7boiOZVjNxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm  # pour des barres de progression\n",
        "\n",
        "# optimiseur\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Boucle d'entraînement\n",
        "num_epochs = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "        loss = loss_fct(outputs.view(-1, num_labels), labels.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss/len(dataloader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlgT_3eGk9Qs",
        "outputId": "475fc651-34b2-4698-caaf-ace594a7ef4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/4 [00:00<?, ?it/s]<ipython-input-4-66bef5b07057>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[0]) for key, val in encoding.items()}\n",
            "100%|██████████| 4/4 [00:48<00:00, 12.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.9773187972605228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:42<00:00, 10.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3, Loss: 0.05642773490399122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:41<00:00, 10.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Loss: 0.04429928492754698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_entities(text, model, tokenizer, label_map):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = torch.tensor([input_ids]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=(input_ids != 0).unsqueeze(0))\n",
        "        logits = outputs[0]\n",
        "\n",
        "    # labels_idx est unidimensionnel\n",
        "    labels_idx = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()  # Utilisez squeeze() pour réduire les dimensions superflues\n",
        "\n",
        "    # S'assurer que labels_idx est itérable (en le convertissant en liste)\n",
        "    if labels_idx.ndim == 0:\n",
        "        labels_idx = [labels_idx]\n",
        "\n",
        "    # indices en labels\n",
        "    inv_label_map = {v: k for k, v in label_map.items()}\n",
        "    predicted_labels = [inv_label_map[idx] for idx in labels_idx]\n",
        "\n",
        "    # Associer les tokens aux labels prédits\n",
        "    return list(zip(tokens, predicted_labels))\n",
        "\n",
        "\n",
        "# Exemple d'utilisation\n",
        "text = \"Charlie Parker was prescribed Aspirin for his headaches.\"\n",
        "predicted_entities = predict_entities(text, model, tokenizer, label_map)\n",
        "print(predicted_entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT4rJ3-clB2h",
        "outputId": "9a30d072-3d39-436e-9310-9ccde9a977e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('charlie', 'O'), ('parker', 'O'), ('was', 'O'), ('prescribed', 'O'), ('as', 'O'), ('##pi', 'O'), ('##rin', 'O'), ('for', 'O'), ('his', 'O'), ('headache', 'O'), ('##s', 'O'), ('.', 'O')]\n"
          ]
        }
      ]
    }
  ]
}